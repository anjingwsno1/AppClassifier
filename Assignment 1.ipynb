{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment 1.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"xSLnEudzPmgt","colab_type":"text"},"cell_type":"markdown","source":["# COMP5318 - Machine Learning and Data Mining - S1 2018"]},{"metadata":{"id":"slFYYixdUOg1","colab_type":"text"},"cell_type":"markdown","source":["### Group Number: 38\n","### Tutor Name:Kelvin & Anthony & Harrison "]},{"metadata":{"id":"QSeeeotjP54F","colab_type":"text"},"cell_type":"markdown","source":["## Assignment 1 - Due 07 May 2018, 5:00pm"]},{"metadata":{"id":"fIvhv-pSQZPL","colab_type":"text"},"cell_type":"markdown","source":["Please refer to [Description File](https://drive.google.com/open?id=1fUFTqcjbSR75tiS48imwjlEVxZVS6U42YElwoVPexwY) for information about the assignment.\n","\n"]},{"metadata":{"id":"KcYZJPr8frtm","colab_type":"text"},"cell_type":"markdown","source":["#### Group Members (name and student number):\n","\n","\n","*   Heng Guo 460199841\n","*   Haoxing Wu 460133111\n","*   Shen Wang 460255642 \n","\n"]},{"metadata":{"id":"nOWmWCq3gEwF","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","\n"]},{"metadata":{"id":"Dsk4FUnOUx3s","colab_type":"text"},"cell_type":"markdown","source":["## Authenticate and create PyDrive client."]},{"metadata":{"id":"OcFUG6SMVzkY","colab_type":"text"},"cell_type":"markdown","source":["You will be prompted with a link to click on, and give permission to Google Colab to access your Google Drive. If you don't want to give permission to your personal google drive, create a new gmail account, and complete this process using the new account."]},{"metadata":{"id":"Z_FCt_RtAnpE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!pip install -U -q PyDrive\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# 1. Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","# PyDrive reference:\n","# https://googledrive.github.io/PyDrive/docs/build/html/index.html\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eaAbndQYVTKB","colab_type":"text"},"cell_type":"markdown","source":["## Import data\n","Locate the \"Data\" folder in your drive. Right click and click \"share\" to get the ID of the folder. Replace < Data folder id > with the id you got. (id should look like \"1j8oG_vCmum965Ghg8LdbSkfj-lfi-AZ0\" )"]},{"metadata":{"id":"oknGSWjHNCmO","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import pandas as pd\n","import io\n","import time\n","import numpy as np"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A2siYFZuM2JJ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":102},"outputId":"d032f761-aecc-4d6d-c587-44ce03cc4afe","executionInfo":{"status":"ok","timestamp":1525655327488,"user_tz":-600,"elapsed":1249,"user":{"displayName":"wu haoxing","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100641581970918820128"}}},"cell_type":"code","source":["file_list = drive.ListFile({'q': \"'1H5y1oS5EmjO9lhNCwKzOg4ciQBS2B14j' in parents and trashed=false\"}).GetList()\n","for file1 in file_list:\n","  print('title: %s, id: %s' % (file1['title'], file1['id']))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["title: predicted_labels.csv, id: 1qKwq4fKd8bOw79BpAEO4uaZFaxfD5KJF\n","title: training_data.csv, id: 1SBp1WGc1lHpRwy0N8qnOvU9q6kSu8Lnh\n","title: test_data.csv, id: 1TG_-UlxBI03TAensV4GPYbszzwqRlNJ-\n","title: training_desc.csv, id: 1VxhawDsD_lunMDEsNF4O1Vutudc6HNpT\n","title: training_labels.csv, id: 1zd8iyfFU3iSHy3ypU8CBnODxoxdPzrnZ\n"],"name":"stdout"}]},{"metadata":{"id":"iGOTPZQWWlf8","colab_type":"text"},"cell_type":"markdown","source":["### Pulling data into Google Colab."]},{"metadata":{"id":"vo17S6sITJfa","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["training_data_downloaded = drive.CreateFile({'id': '1SBp1WGc1lHpRwy0N8qnOvU9q6kSu8Lnh'})\n","training_data_downloaded.GetContentFile('training_data.csv')\n","\n","training_desc_downloaded = drive.CreateFile({'id': '1VxhawDsD_lunMDEsNF4O1Vutudc6HNpT'})\n","training_desc_downloaded.GetContentFile('training_desc.csv')\n","\n","training_labels_downloaded = drive.CreateFile({'id': '1zd8iyfFU3iSHy3ypU8CBnODxoxdPzrnZ'})\n","training_labels_downloaded.GetContentFile('training_labels.csv')\n","\n","test_data_downloaded = drive.CreateFile({'id': '1TG_-UlxBI03TAensV4GPYbszzwqRlNJ-'})\n","test_data_downloaded.GetContentFile('test_data.csv') \n","\n","test_data_downloaded = drive.CreateFile({'id': '1-eGhlM2cVE4m3e1BNqrsYSyIPHVl5ylk'})\n","test_data_downloaded.GetContentFile('predicted_labels.csv') "],"execution_count":0,"outputs":[]},{"metadata":{"id":"QhI6TXRtWsl7","colab_type":"text"},"cell_type":"markdown","source":["### Load data files\n","(example:)"]},{"metadata":{"id":"saVLVnGvT59I","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["training_file = open('training_data.csv').readlines()\n","label_file = open('training_labels.csv').readlines()\n","test_file = open('test_data.csv').readlines()\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X_9yMelvf_GK","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n"]},{"metadata":{"id":"O5zfbjqlgSWN","colab_type":"text"},"cell_type":"markdown","source":["## A. Introduction (max 500 words)\n"]},{"metadata":{"id":"N6KtBHrPgiCB","colab_type":"text"},"cell_type":"markdown","source":["1- What is the aim of the study?\n"]},{"metadata":{"id":"WP6x1GfHgrX1","colab_type":"text"},"cell_type":"markdown","source":["The aim of this study is using a suitable classifier for apps sort. We will classify these apps to different group based on their description. There are huge amounts of apps and their description, we need to choose suitable pre-processing operation to decrease the load of computation. There are several important indicators (accuracy, processing time, precision, recall and f-measure) for evaluation performance. To get higher accuracy results and achieve high performance, the selection and apply of classifier are significant."]},{"metadata":{"id":"GRYT8q82g1uv","colab_type":"text"},"cell_type":"markdown","source":["2- Why is this study important?"]},{"metadata":{"id":"W03lryJrg-EX","colab_type":"text"},"cell_type":"markdown","source":["In the process of data processing, it is very important to reduce data and adjust the structure and format of data. Because computers are different in dealing with data with different types of data processing methods and speed, the format of data has great influence on the operation of programs. Obviously, the larger the amount of data is, the longer the processing time will be. Reducing the amount of data helps to improve user experience and save storage space, which means improving economic efficiency.\n","In the face of today's massive information, the performance of classifiers is an important factor for information retrieval performance. Different classifier algorithms use different methods and processes, and the final results may be different. By completing this task, we need to try different classifiers. This has greatly improved our ability to analyze data and understand the use of classifiers.\n","\n"]},{"metadata":{"id":"Cgh2I0MUS-NV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"vXjExeGHhfdL","colab_type":"text"},"cell_type":"markdown","source":["## B. Methods (max 1500 words)"]},{"metadata":{"id":"l78VSnashn5y","colab_type":"text"},"cell_type":"markdown","source":["### Pre-processing (if any)\n","\n","*You've been given a few examples of pre-processing during lectues, such as normalisation, removal of outliers and so on. While you are allowed to use external libraries for optimization and linear algebraic calculations, you are NOT allowed to use external libraries for  pre-processing. If you decide to use any form of pre-processing, explain your method clearly here and then write the code for it.*"]},{"metadata":{"id":"UdWHc8SLh3m7","colab_type":"text"},"cell_type":"markdown","source":["The format of Apps name and label is str. This kind of format need more time and space to process. Therefore, we give each label a number to represent them. There are 30 labels. Then, we use these numbers to represent feature vectors. Finally, we got the matrix, include all feature vectors belong to this label. it is convenient for computation and processing. We have tryed in different Python Environmental, the results is native Python has best performance compare with pandas and reand_csv.\n","After that, we tested 2 functions, one is the PCA, another is to filter smaller data.\n","\n","**PCA**, as a popular procedure to convert huge data into a set of principals, smaller components. First, use each value minus average mean of its column. Then calculate the covariances matrix as well as its eigenvalues and eigenvectors. Then sort the eigenvalue in descending order and get the largest K eigenvalues and its index. Using the data matrix to dot the transposition of the k eigenvectors and get the reduced matrix. However, we found out that it cost too much time to calculate the eigenvalues and eigenvectors for a 13626*13626 matrix as well as the dot multiplication. The test shows that it cost 1328 seconds for the pre-processing module. It did increase the speed of classifier for about 7 times and decrease only a little bit of accuracy for about 0.007. Therefore, with such a long pre-processing time, we choose to not use it for the final processing.\n","\n","Figure 1. The result of using PCA https://drive.google.com/open?id=1RxVlSeD0pxoCpnujj49wRshKM8AE67o_\n","\n","**Filter**, In training_data.csv, there are many tf-idf values of words for each app’s name. We consider find a way which based on these tf-idf values to reduce the size of dataset. Firstly, we need know what is tf-idf. TF-IDF (term frequency–inverse document frequency) is a technique which used for data mining and information retrieval. Tf-idf is used to measure the importance of a word in a folder or a file. There is a proportional relationship between the number of occurrences in file and the importance of this word. Search engines always use Tf-idf weighted to evaluate the interdependency between file and information which consumers wants to find. \n","Therefore, based on this method, delete a part of columns which the value of tf-idf value are extremely low is a feasible way to reduce size of dataset. Because, the extremely low value means that this word is not important for retrieval. \n","First we analyzed the original data, we calculate the average mean for each column, sorted them and plot it in a picture as figure 2. It shows that the mean suddenly increased in a period time between 0 to 0.001, so we tested several times and found out the suitable threshold is 0.0001. So, we delete those columns whose average means are less than 0.0001, there are about 7457 columns which meet the requirement. The result also shows that it only needs 8 more seconds for the filter and increase the speed for about 4 times. Moreover, the accuracy has nearly no decreasing.\n","\n","Figure 2. Filter https://drive.google.com/open?id=1C8AP6DBtvlykihf4idtWO17Yd3Bu2jmV\n"]},{"metadata":{"id":"oWvHtQ4Shlmr","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":51},"outputId":"31bca667-929b-4e42-fb94-d2e7702a87e6","executionInfo":{"status":"ok","timestamp":1525655740253,"user_tz":-600,"elapsed":48296,"user":{"displayName":"wu haoxing","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100641581970918820128"}}},"cell_type":"code","source":["app_name = {}\n","label = []\n","training_list = [[]]*10\n","test_data = []\n","deleteColumn = []\n","\n","def init():\n","    start=time.time()\n","    global training_file,label_file,test_file,app_name,deleteColumn\n","    extra = len(training_file)%10\n","    line_num = int(len(training_file)/10)\n","    line_list = [0]*10\n","    for i in range(0,10):\n","        if i < extra:\n","            line_list[i] = line_num+1\n","        else:\n","            line_list[i] = line_num\n","    for l in label_file:\n","        l = l.replace('\\n','').split(',')\n","        app_name[l[0]]=l[1]\n","    for l in app_name.values():\n","        if l not in label:\n","            label.append(l)\n","    label_file = []\n","\n","    temp = []\n","    for l in training_file:\n","        l = l.replace('\\n','').split(',')\n","        l[0] = label.index(app_name[l[0]])\n","        temp.append(l)\n","    count = 0\n","    \n","    '''\n","    ##commit PCA\n","    trainingMatrix = np.array(temp, dtype=np.float)\n","    training_matrix = np.delete(trainingMatrix,0,1)\n","    row, column=training_matrix.shape\n","    k = 450\n","    for i in range(0,column):\n","        training_matrix[:,i] = training_matrix[:,i]-np.sum(training_matrix[:,i])/row\n","    cov_matrix = np.cov(training_matrix.T)\n","    eigValue,eigVect = np.linalg.eig(cov_matrix)\n","    eigValIndice = np.argsort(eigValue)\n","    k_eigValIndice = eigValIndice[-1:-(k+1):-1]   \n","    k_eigVect = eigVect[:,k_eigValIndice]         \n","    lowMatrix = training_matrix.dot(k_eigVect)\n","    temp = np.insert(lowMatrix, 0, values=trainingMatrix[:,0].T, axis=1)\n","    \n","    ##commit end\n","    '''\n","    ##commit delete\n","    trainingMatrix = np.array(temp, dtype=np.float)\n","    training_matrix = np.delete(trainingMatrix,0,1)\n","    row, column = training_matrix.shape\n","    for j in range(0,column):\n","        if (np.sum(training_matrix[:,j])/row)<0.0001:\n","            deleteColumn.append(j)\n","    lowDataMatrix = np.delete(training_matrix, deleteColumn, axis = 1)\n","    temp = np.insert(lowDataMatrix, 0, values=trainingMatrix[:,0].T, axis=1)\n","    print('delete {} columns'.format(len(deleteColumn)))\n","    ##commit end\n","    \n","    training_file = []\n","    for i in range(0,10):\n","        training_list[i] = np.array(temp[count:count+line_list[i]],dtype=np.float)\n","        count+=line_list[i]\n","    temp=[]\n","    app_name=[]\n","    print('init time:',time.time()-start)\n","    \n","init()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["delete 7457 columns\n","init time: 47.631765604019165\n"],"name":"stdout"}]},{"metadata":{"id":"P1_hsOaLhfTf","colab_type":"text"},"cell_type":"markdown","source":["### Classifier"]},{"metadata":{"id":"pnvIjGhUh6Mp","colab_type":"text"},"cell_type":"markdown","source":["*You have to implement the classiﬁer yourself. Same as with pre-processing, you are not allowed to use external libraries for classification. In this section, clearly explain your classifier followed by your code. Your description for pre-processing and classifier should not be more than 1500 words in total.*"]},{"metadata":{"id":"Os1-qbDXeH61","colab_type":"text"},"cell_type":"markdown","source":["For both classifier, we use 10 fold cross validation to calculate the average accuracy.\n","\n","**Naïve Bayes**\n","\n","Naïve Bayes is one of the classical machine learning algorithms, and it is also one of the few classification algorithms based on probability theory. Naive Bayes principle is simple and easy to implement. It is mostly used for text categorization, such as spam filtering.\n","The core of naive Bayes is Bayes rule, and the cornerstone of Bayes rule is conditional probability.\n","\n","Figure 7 Bayes Rule https://drive.google.com/open?id=1S3RDjL60sbcX1Cbh8S_rbWkTPJfIGFoE\n","\n","Firstly, we divide the feature vectors into 30 categories according to the label. Because the matrix is sparse matrix, we use 1e-300 represent 0.\n","Using an dict struct to store the data, key is the label and values are feature vetores and each line represent for one app. Based on Bayes rule, for each label matrix, count how many apps in it and divided by the total number of apps as P(y)=F(y)/N. In each label, the column is one features as Xi, sum the coulumn and then divided by the total number of features as P(xi|y). So far, we have the P(y) and P(x|y) and can predict.\n","To avoid underflow, we take logarithms of the product of probability.\n","\n","**Logistic regression**\n","\n","Firstly, to facilitate the next operation, using a dict struct to store the data, key is the label and values are feature vetores and each line represent for one app. \n","Logistic regression is method that when deal with a regression or classification problem, build a cost function, then the optimal model parameters are solved iteratively by the optimization method. Finally, to checkout if our method is useful by testing and verification. Actually, logical regression is a classification method. Usually, use this method to deal with problems which has two kings of output, each one representing one category. Logical regression is always used to solve probability problem.\n","\n","Figure 3 Logistic Regression https://drive.google.com/open?id=1-l21RhA6FSOcFKk3ot_cZAqnvknTh_IO\n","\n","Figure 4 The function diagram of sigmoid https://drive.google.com/open?id=1bbYaAWAjOgjabcOqQzQP_4qoVCoP44k1\n","\n","The figure 4 shows that sigmoid’s output is between (0,1), 0.5 is the intermediate number. Therefore, hϴ(x) is between 0 to 1. If hϴ(x)<0.5 means this data belong to same type. If hϴ>0.5 means this data belong to another type.\n","Based on this formula, we can estimate the parameter ϴ. The meaning of ϴ is the probability when h ϴ(x)=1, thus, for input x classification and results is:\n","\n","                            P(y=1|x;0) =h ϴ(x)\n","                            \n","                            P(y=0|x;0) = 1- h ϴ(x)\n","\n","According to these equations, we use maximum likelihood estimation to get the solution of cost function. \n","\n","\n","Figure 5 Maximum likelihood https://drive.google.com/open?id=1-l21RhA6FSOcFKk3ot_cZAqnvknTh_IO\n","\n","To calculate the result, Gradient Descent is an effective method, we use this method to get the best solution.\n","\n","\n"]},{"metadata":{"id":"AjDQ8JYwhxHl","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class NB:\n","    def __init__(self):\n","        self.x_prop = {}\n","        self.y_prop = {}\n","        self.matrix = np.zeros((30,30))\n","        \n","    ##dict:{[label1:[x1,x2,x3...xn],[x1,x2,...]], [label2:....],...}\n","    def separate(dataset):\n","        separate = {}\n","        for i in range(len(dataset)):\n","            vector = dataset[i]\n","            label = int(vector[0])\n","            if label not in separate:\n","                separate[label] = []\n","            separate[label].append(np.array(vector[1:]))\n","        return separate\n","    \n","    def calculate(dataset):\n","##        f = np.array([sum(x) for x in zip(*dataset)])\n","##        N = np.array(dataset).sum()\n","##        prob = f/N\n","        prob = np.array([sum(x) for x in zip(*dataset)])\n","        prob = np.where(prob==0,1e-300,prob)\n","        return np.log(prob)\n","        \n","    def predict(self,dataset):\n","        p = {}\n","        prediction = []\n","        N = len(dataset)\n","        c = 0\n","        for i in range(0,len(dataset)):\n","            label = dataset[i][0]\n","            vector = np.array(dataset[i][1:],dtype=np.float)\n","\n","            for key in list(self.x_prop.keys()):\n","                p[key] = sum(vector*self.x_prop[key])+self.y_prop[key]\n","            p_t = {k:v for v,k in p.items()}\n","\n","            prediction.append(p_t[max(p.values())])\n","        for i in range(len(dataset)):\n","            self.matrix[int(dataset[i][0])][int(prediction[i])] += 1\n","            if prediction[i] == dataset[i][0]:\n","                c+=1\n","##        print(prediction)\n","        acc = c/N\n","        print('Accuracy ={}'.format(c/N))\n","        return acc\n","        \n","    def train(self):\n","        start = time.time()\n","        test = []\n","        label_feature = {}\n","        acclist=[]\n","        global training_list\n","        \n","        ##Ten-fold-cross-validation\n","        for i in range(0,10):\n","             for j in range(0,10):\n","                 if j == i:\n","                     test = training_list[j]\n","                 else:\n","                     label_feature = NB.separate(training_list[j])\n","                     label_list = list(label_feature.keys())\n","                     for key in label_list:\n","                         if key in self.x_prop.keys():\n","                             self.x_prop[key] += np.array(NB.calculate(label_feature[key]))\n","                         else:\n","                             self.x_prop[key] = np.array(NB.calculate(label_feature[key]))\n","\n","                     for key in label_list:\n","                         if key in self.y_prop.keys():\n","                             self.y_prop[key] += len(label_feature[key])\n","                         else:\n","                             self.y_prop[key] = len(label_feature[key])\n","             N = sum(a.y_prop.values())\n","             for key in list(self.y_prop.keys()):\n","                 self.y_prop[key] = np.log(self.y_prop[key]/N)\n","             acclist.append(NB.predict(self,test))\n","             if i < 9:\n","                 self.x_prop = {}\n","                 self.y_prop = {}\n","        end = time.time()\n","        print('average accuracy =',np.array(acclist).mean())\n","        print('runing time:',end-start)\n","        label_feature = NB.separate(training_list[9])\n","        label_list = list(label_feature.keys())\n","        for key in label_list:\n","            if key in self.x_prop.keys():\n","                self.x_prop[key] += np.array(NB.calculate(label_feature[key]))\n","            else:\n","                 self.x_prop[key] = np.array(NB.calculate(label_feature[key]))\n","\n","        for key in label_list:\n","            if key in self.y_prop.keys():\n","                self.y_prop[key] += len(label_feature[key])\n","            else:self.y_prop[key] = len(label_feature[key])\n","\n","\n","class LR:\n","    def __init__(self):\n","        self.weights = {}\n","        self.matrix = np.zeros((30,30))\n","        self.prediction = []\n","    ##dict:{[label1:[x1,x2,x3...xn],[x1,x2,...]], [label2:....],...}\n","    def separate(dataset):\n","        separate = {}\n","        for i in range(len(dataset)):\n","            vector = dataset[i]\n","            label = int(vector[0])\n","            if label not in separate:\n","                separate[label] = []\n","            separate[label].append(np.array(vector[1:]))\n","        return separate\n","    \n","    def sigmoid(x):\n","        return 1.0 / (1 + np.exp(-x))\n","        \n","    def lr(self,dataset,w):\n","        m,n = np.shape(dataset)\n","        max_iter = 10\n","        y = 1\n","        for j in range(max_iter):\n","            dataIndex=list(range(m))\n","            for i in range(m):\n","                alpha=4/(1+i+j)+0.001\n","                randIndex=int(np.random.uniform(0,len(dataIndex)))\n","                h = LR.sigmoid(dataset[randIndex] * w)\n","                error= y - h\n","                w = w + alpha*dataset[randIndex]*error\n","                del(dataIndex[randIndex])\n","        return w \n","\n","    def train(self):\n","        acclist = []\n","        global training_list\n","        start = time.time()\n","        ##Ten-fold-cross-validation\n","        for i in range(0,10):\n","             for j in range(0,10):\n","                 if j == i:\n","                     test = training_list[j]\n","                 else:\n","                     label_feature = LR.separate(training_list[j])\n","                     for key in label_feature:\n","                         if key not in self.weights:\n","                             w = np.ones((len(training_list[j][0])-1))\n","                             self.weights[key]=LR.lr(self,label_feature[key],w)\n","                         else:\n","                             self.weights[key]=LR.lr(self,label_feature[key],self.weights[key])\n","             acclist.append(LR.predict(self,test))\n","             print('accuracy:',acclist[i])\n","             if i < 9:\n","                 self.weights = {}\n","        end = time.time()\n","        print('average accuracy =',np.array(acclist).mean())\n","        print('runing time:',end-start)\n","        label_feature = LR.separate(training_list[9])\n","        for key in label_feature:\n","            if key not in self.weights:\n","                w = np.ones((len(training_list[j][0])-1))\n","                self.weights[key]=LR.lr(self,label_feature[key],w)\n","            else:\n","                self.weights[key]=LR.lr(self,label_feature[key],self.weights[key])\n","    def predict(self,dataset):\n","        global label\n","        prediction = []\n","        p = {}\n","        c = 0\n","        N = len(dataset)\n","        for i in range(len(dataset)):\n","            xi = np.array(dataset[i][1:])\n","            for key in self.weights:\n","                p[key] = float(xi.dot(self.weights[key]))\n","            p_t = {k:v for v,k in p.items()}\n","            prediction.append(p_t[max(p.values())])\n","        for i in range(len(dataset)):\n","            self.matrix[int(dataset[i][0])][int(prediction[i])] += 1\n","            if prediction[i] == dataset[i][0]:\n","                c+=1\n","##        print(prediction)\n","        self.prediction = prediction\n","        for i in range(len(self.prediction)):\n","            self.prediction[i] = label[int(self.prediction[i])]\n","        acc = c/N\n","##        print('Accuracy ={}'.format(c/N))\n","        return acc"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ev9zV335hv0m","colab_type":"text"},"cell_type":"markdown","source":["## C. Experiments and Results (max 500 words)"]},{"metadata":{"id":"GrOvQk1krsRP","colab_type":"text"},"cell_type":"markdown","source":["*We expect you to provide a rigorous performance evaluation. To provide an estimate of the performance (precision, recall, F-measure, etc.) of your classiﬁer in the report, you can perform a 10-fold cross validation on the training set provided and average the metrics for each fold.*"]},{"metadata":{"id":"hWFnjiN2iG1M","colab_type":"text"},"cell_type":"markdown","source":["### Accuracy"]},{"metadata":{"id":"2ggc68nniKM8","colab_type":"text"},"cell_type":"markdown","source":["We have used ten-fold cross validation to test accuracy. This method is an effective way for precision testing. We divided the data set into ten parts, 9 of all dataset are trained and use one part of dataset for testing. Then, each part of dataset to do once testing, others as training data. After that, calculate the mean value of all outcomes. This result represents our estimation of the accuracy of the algorithm. If a higher precise is needed, this process could be executed many times. In our program, we did 10 times 10 times cross validation. The average accuracy is 0.6103277248763641. Runing time is 217.59455585479736s."]},{"metadata":{"id":"Ly0I30PriU-s","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":221},"outputId":"d84c1129-2633-45fd-c030-67fea26bb183","executionInfo":{"status":"ok","timestamp":1525656156261,"user_tz":-600,"elapsed":221394,"user":{"displayName":"wu haoxing","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100641581970918820128"}}},"cell_type":"code","source":["if __name__ == '__main__':\n","##    init()\n","##    a=NB()\n","##    a.train()\n","    b=LR()\n","    b.train()\n","    appname = []\n","    test_data = []\n","    for l in test_file:\n","        l = l.replace('\\n','').split(',')\n","        appname.append(l[0])\n","        l[0] = 0\n","        test_data.append(l)\n","    "],"execution_count":9,"outputs":[{"output_type":"stream","text":["accuracy: 0.6136250621581303\n","accuracy: 0.6016907011437096\n","accuracy: 0.6001989060169071\n","accuracy: 0.5972153157633019\n","accuracy: 0.5990049751243781\n","accuracy: 0.6169154228855721\n","accuracy: 0.6268656716417911\n","accuracy: 0.618407960199005\n","accuracy: 0.6114427860696517\n","accuracy: 0.6179104477611941\n","average accuracy = 0.6103277248763641\n","runing time: 217.59455585479736\n"],"name":"stdout"}]},{"metadata":{"id":"y_xa-3K1iUS_","colab_type":"text"},"cell_type":"markdown","source":["### Extensive Analysis\n","\n"]},{"metadata":{"id":"avGE-c9zieOc","colab_type":"text"},"cell_type":"markdown","source":["There are 30 categories in the data, so this part is basically using a 30*30 confusion matrix to calculate the precision, recall, f-measure and the accuracy. Based on the definition of these concepts, the matrix uses the column as predictions and the row as golden standards. the equations are as followed:\n","\n","Precision = right number / the total number of its column\n","\n","Recall = right number / the total number of its row\n","\n","F-measure = 2*right number / (the total number of its column+row)\n","\n","Accuracy = right number/total number\n","\n","All of precision, recall and f-measure are calculated separately for each class and then the result shows their average mean.\n"]},{"metadata":{"id":"IEl4Jjo6ifu1","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":1190},"outputId":"acf110f0-ae48-4f2d-8305-4cae89449ae9","executionInfo":{"status":"ok","timestamp":1525657473858,"user_tz":-600,"elapsed":880,"user":{"displayName":"wu haoxing","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100641581970918820128"}}},"cell_type":"code","source":["  def getCost(matrix_30_30) :\n","      precision_total=0\n","      recall_total=0\n","      f_meature_total=0\n","      precision_average=0\n","      recall_average=0\n","      f_meature_average=0\n","      right=0\n","      total=np.sum(matrix_30_30)\n","\n","      for i in range(30):\n","          precision_total+= matrix_30_30[i][i]/(np.sum(matrix_30_30[:,i]))\n","          recall_total+= matrix_30_30[i][i]/(np.sum(matrix_30_30[i]))\n","          f_meature_total+= 2*matrix_30_30[i][i]/(np.sum(matrix_30_30[:,i])+np.sum(matrix_30_30[i]))\n","          right+=matrix_30_30[i][i]\n","      precision_average = precision_total/30\n","      recall_average = recall_total/30\n","      f_meature_average = f_meature_total/30\n","      accuracy = right/total\n","      print ('Average precision is : {}\\nAverage recall is : {}\\nAverage f-meature is : {}\\nAccuracy is : {}'.format(precision_average, recall_average, f_meature_average, accuracy))\n","      ##return (precision_average, recall_average, f_meature_average, accuracy)\n","      \n","  b.matrix = pd.DataFrame(b.matrix)\n","  print(b.matrix)\n","  getCost(b.matrix.values)\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["       0      1      2      3      4      5      6      7      8      9   \\\n","0   637.0    4.0    0.0    2.0    0.0    0.0    2.0    0.0    4.0    3.0   \n","1    66.0  383.0    1.0    1.0    6.0   66.0   11.0    6.0    6.0    4.0   \n","2    50.0    4.0  148.0    1.0    8.0   21.0    6.0   14.0   31.0    8.0   \n","3     3.0    0.0    1.0  419.0    0.0    0.0    1.0    1.0    3.0   44.0   \n","4     6.0    7.0    0.0    3.0  525.0    3.0    6.0   32.0    8.0    4.0   \n","5     8.0   54.0    2.0    0.0    0.0  426.0    5.0    9.0   36.0    1.0   \n","6     9.0    6.0    0.0    0.0    2.0    5.0  589.0    0.0    1.0    1.0   \n","7     2.0    2.0    0.0    3.0    4.0    5.0    1.0  597.0    9.0    0.0   \n","8    26.0    4.0    1.0    3.0    4.0   75.0    7.0   10.0  262.0    5.0   \n","9     6.0    0.0    0.0   42.0    1.0    1.0    0.0    6.0    0.0  528.0   \n","10   23.0    3.0    0.0    1.0    5.0    4.0    3.0   17.0    4.0   43.0   \n","11    4.0    7.0    0.0    1.0    3.0    6.0    2.0   15.0    2.0    1.0   \n","12   22.0    0.0    0.0    3.0    1.0    4.0    2.0   17.0   12.0    0.0   \n","13    9.0    0.0    0.0   47.0    1.0    0.0    0.0    2.0    0.0   16.0   \n","14    6.0    1.0    1.0    0.0   70.0    7.0    3.0   12.0    1.0    4.0   \n","15   19.0    2.0    1.0  104.0    3.0    3.0    1.0   14.0    0.0  116.0   \n","16    1.0    6.0    0.0    0.0    1.0   11.0   15.0    2.0    7.0    2.0   \n","17  104.0   24.0    0.0   23.0   12.0   21.0    4.0   66.0   13.0   17.0   \n","18   28.0   10.0    3.0    1.0    8.0   61.0   18.0    5.0   85.0    0.0   \n","19    7.0   16.0    0.0   13.0   22.0   14.0    8.0   17.0    4.0    6.0   \n","20   37.0    9.0    1.0   25.0    1.0    1.0    2.0   11.0    4.0    3.0   \n","21   90.0    7.0    3.0    1.0    1.0    9.0    2.0  114.0   10.0    2.0   \n","22    4.0    3.0    0.0    4.0   20.0    3.0    3.0    7.0    4.0    2.0   \n","23    3.0    0.0    0.0    7.0    0.0    0.0    1.0    3.0    0.0   18.0   \n","24   10.0   15.0    5.0    1.0    9.0   36.0   51.0    6.0   21.0    3.0   \n","25    6.0    2.0    0.0    0.0    3.0   12.0    0.0    1.0    7.0    0.0   \n","26    0.0    0.0    0.0    0.0    2.0    2.0   13.0    0.0    2.0    0.0   \n","27   54.0   37.0    1.0    2.0   50.0   16.0   99.0   30.0   33.0    1.0   \n","28    2.0    0.0    1.0    0.0    0.0    3.0    3.0    0.0    2.0    0.0   \n","29    4.0    0.0    0.0   38.0    1.0    0.0    2.0    4.0    0.0   14.0   \n","\n","    ...      20     21     22     23     24     25     26    27     28     29  \n","0   ...     0.0   12.0    0.0    0.0    0.0    1.0    1.0   0.0    4.0    1.0  \n","1   ...     1.0   10.0    2.0    5.0    9.0    6.0    7.0  10.0    4.0    0.0  \n","2   ...     0.0   14.0    0.0    2.0   12.0    0.0   13.0   1.0    6.0    1.0  \n","3   ...     0.0    3.0    0.0   19.0    0.0    1.0    1.0   0.0    0.0   24.0  \n","4   ...     0.0    3.0    1.0    1.0    0.0   11.0    8.0  12.0    5.0    1.0  \n","5   ...     1.0   19.0    0.0    1.0   14.0    3.0   14.0   0.0    2.0    0.0  \n","6   ...     0.0    3.0    1.0    3.0   12.0    9.0   29.0   4.0    3.0    0.0  \n","7   ...     0.0   30.0    4.0    1.0    0.0    0.0    3.0   2.0    0.0    0.0  \n","8   ...     0.0   36.0    2.0    4.0    9.0   10.0   25.0   2.0   14.0    0.0  \n","9   ...     0.0    3.0    1.0   32.0    0.0    0.0    1.0   0.0    0.0    2.0  \n","10  ...     0.0   11.0    2.0    1.0    8.0    4.0    9.0   5.0    8.0    0.0  \n","11  ...     0.0    4.0   14.0    8.0    9.0    5.0    4.0   2.0   27.0    1.0  \n","12  ...     0.0    1.0    1.0    0.0    0.0    1.0    0.0   0.0    4.0    0.0  \n","13  ...     0.0    0.0    2.0    4.0    0.0    0.0    1.0   0.0    0.0    7.0  \n","14  ...     0.0    1.0    1.0    2.0    3.0    0.0    4.0   1.0    0.0    0.0  \n","15  ...     0.0    3.0    1.0   33.0    0.0    0.0    0.0   1.0    0.0   15.0  \n","16  ...     0.0    2.0    1.0    2.0    4.0  147.0   11.0   1.0    9.0    1.0  \n","17  ...     4.0  114.0    9.0   10.0    1.0    0.0    3.0   6.0    7.0    2.0  \n","18  ...     0.0   28.0    0.0    0.0   32.0    5.0   27.0   1.0    6.0    0.0  \n","19  ...     0.0    6.0    7.0    7.0    7.0    5.0    7.0  20.0    7.0    0.0  \n","20  ...    89.0    6.0    1.0    0.0    1.0    1.0    1.0   0.0    3.0    0.0  \n","21  ...     1.0  420.0    5.0    0.0    1.0    0.0    0.0   1.0    2.0    0.0  \n","22  ...     0.0   13.0  371.0   14.0    1.0    9.0    0.0   0.0   18.0   18.0  \n","23  ...     0.0    2.0    2.0  649.0    0.0    0.0    0.0   0.0    0.0    1.0  \n","24  ...     0.0   18.0    0.0    2.0  240.0   22.0   99.0   1.0    0.0    1.0  \n","25  ...     0.0    3.0    0.0    0.0   12.0  507.0   17.0   1.0   11.0    0.0  \n","26  ...     0.0    0.0    0.0    4.0    4.0    8.0  637.0   0.0    3.0    0.0  \n","27  ...     0.0   13.0    6.0   15.0   13.0   14.0   18.0  76.0   11.0    1.0  \n","28  ...     0.0    0.0    1.0    0.0    1.0    1.0    2.0   0.0  419.0    0.0  \n","29  ...     0.0    0.0   37.0   16.0    1.0    0.0    1.0   0.0    0.0  239.0  \n","\n","[30 rows x 30 columns]\n","Average precision is : 0.6313738037309631\n","Average recall is : 0.6027390885912247\n","Average f-meature is : 0.5884539490248486\n","Accuracy is : 0.6103263032232391\n"],"name":"stdout"}]},{"metadata":{"id":"NHyWvxoDtRdh","colab_type":"text"},"cell_type":"markdown","source":["## D. Export Results "]},{"metadata":{"id":"ztOFuwuFlIOB","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":34},"outputId":"426b0dcc-736c-4555-f67a-322d120c9359","executionInfo":{"status":"ok","timestamp":1525623898928,"user_tz":-600,"elapsed":5671,"user":{"displayName":"heng guo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"103403217236429644864"}}},"cell_type":"code","source":["    test_data = np.array(test_data,dtype=np.float)\n","    test_data = np.delete(test_data,deleteColumn,axis=1)\n","    b.predict(np.array(test_data,dtype=np.float))\n","    output = pd.DataFrame({'appname':appname,'label':b.prediction})\n","    output.to_csv('predicted_labels.csv',header=False,index=False)\n","    \n","    uploaded = drive.CreateFile({'title': 'predicted_labels.csv'})\n","    uploaded.SetContentFile('predicted_labels.csv')\n","    uploaded.Upload()\n","    print('Uploaded file with ID {}'.format(uploaded.get('id')))"],"execution_count":157,"outputs":[{"output_type":"stream","text":["Uploaded file with ID 1u-j3u0fGVXwGZPUPVUnpeb9fiW2cp_3_\n"],"name":"stdout"}]},{"metadata":{"id":"7SX6tdMdtX53","colab_type":"text"},"cell_type":"markdown","source":["This is important for your grading.\n","\n","\n","You must save a file named “**predicted_labels.csv**” in the **same data format** as “training_labels.csv”.\n","\n","You can use PyDrive to save the data file (example is not provided here and you should find out how to do it on your own).\n","\n","Make sure the predictions (classiﬁcation results for the test set) are in the **same order** as test inputs, i.e. the ﬁrst row of “predicted_labels.csv” corresponds to the ﬁrst row of “test_data.csv” and so on). \n","\n","Your score will be based on how accurate your approach is. We will collect “predicted_labels.csv” and compare it to the actual labels to get the accuracy of your approach. For further testing purposes, we may use a diﬀerent test set while grading.\n"]},{"metadata":{"id":"CBYRlGaCbknL","colab_type":"text"},"cell_type":"markdown","source":["**Insert the url address of your predicted_labels.csv file: https://drive.google.com/file/d/1zd8iyfFU3iSHy3ypU8CBnODxoxdPzrnZ/view?usp=sharing  **\n","\n"]},{"metadata":{"id":"z9tuqCpZigo1","colab_type":"text"},"cell_type":"markdown","source":["## E. Discussion (max 300 words)"]},{"metadata":{"id":"skiNNJW1ikjg","colab_type":"text"},"cell_type":"markdown","source":["*Provide a meaningful and relevant personal reflection.* \n","*Argue around the results from the experiments. Try to explain things like why or why not one classifier performed better than the other in some aspects or why results for a given class were so bad. The discussion is about the results and it should be more technical than philosophical (don't talk about your feelings).*\n"]},{"metadata":{"id":"AD7qJGepfU3r","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"NXwLgIecsBBK","colab_type":"text"},"cell_type":"markdown","source":["In the pre-processing step, we have used different wat try to reduce the size of data and change the format of dataset to improve the efficiency of data processing. The original dataset has many data which the format is string. This type of data is easy to identify for humans, but not efficient for computer operation. \n","\n","Both PCA and Filter has advantages. PCA spend more time on pre-processing, it takes about 20 minutes. However, it could save time on next step. We found that after the treatment of PCA, the running time is about 100s, shorter than filter. For the total time, filter is faster than PCA. Therefore, for this dataset, filter is a better option.\n","\n","Contrast Naïve Bayes and logistic regression, the running time of Naïve Bayes is much longer than logistic regression. The running time of Naïve Bates is about 1402s. The running time of logistic regression is about 217s. The reason is in Naïve Bayes method, we use an extremely small number: 1e-300 to represent 0 in sparse matrix. This setting causes computer spends more time on calculation. For logistic regression, this step is not needed. And another factor is the accuracy of Naïve Bayes is lower than logistic regression. The average accuracy is about 61.033% for logistic regression. The average accuracy of Naïve Bayes is 55.735%. \n","\n","Figure 8 Resultes of Naive Bayes https://drive.google.com/open?id=1F5rsNPVhbRP2tq3morq_87WPEOuREmx6\n"]},{"metadata":{"id":"kViLL0fEirga","colab_type":"text"},"cell_type":"markdown","source":["## F. Conclusion and future work (max 200 words)"]},{"metadata":{"id":"QP7ckuJxjCqh","colab_type":"text"},"cell_type":"markdown","source":["*Provide meaningful conclusions based on your results and suggest meaningful future work.*\n","\n","\n"]},{"metadata":{"id":"Wv-7SL2-sMNT","colab_type":"text"},"cell_type":"markdown","source":["We have achieved the main aim of study by using logistic regression classifier. In pre-processing step, we have tried SPA and filter to deal with original dataset. We have also implemented the function with the Naive Bayes method. Finally, we got a 61.033% accuracy. The running time is about 217.595s. \n","\n","Through the completion of this project, we learned and practiced a variety of data preprocessing and classifier usage. We have gained more understanding of machine learning principles and implementation. Further work is to try different classifiers, such as Support Vector Machines, and also, there are some other methods we need to learn more for descending dimension, such as Singular Value Decomposition. \n","\n","We will try to find some patterns to distinguish which kind of classifier is appropriate for certain type of dataset.There are some factors we need think about: The complexity of the classification function and the size of the training data; The dimension of the input characteristic space; The homogeneity and relationship between input eigenvectors.\n"]},{"metadata":{"id":"il1S9xh5suYH","colab_type":"text"},"cell_type":"markdown","source":["## G. References"]},{"metadata":{"id":"bW7XuBcWs0x2","colab_type":"text"},"cell_type":"markdown","source":["*You should provide appropriate citations throughout your report and list your references here.*"]},{"metadata":{"id":"tB9UVjbwtCRW","colab_type":"text"},"cell_type":"markdown","source":["list of references\n","\n","1.  Raschka, S. (2014). Implementing a Principal Component Analysis (PCA). Retrieved from http://sebastianraschka.com/Articles/2014_pca_step_by_step.html \n","2.  Ray, S. (2017). 6 Easy Steps to Learn Naive Bayes Algorithm (with code in Python). Retrieved from https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/\n","3.  Li, S. (2017). Building A Logistic Regression in Python, Step by Step. Retrieved from https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8\n","4.  Feller, W. (2008). An introduction to probability theory and its applications (Vol. 2). John Wiley & Sons.\n","5.  Murphy, K. P. (2006). Naive bayes classifiers. University of British Columbia, 18."]},{"metadata":{"id":"DzqTre2yu6YJ","colab_type":"text"},"cell_type":"markdown","source":["# Sumbitting your assignment"]},{"metadata":{"id":"mwUw2n7uu9zH","colab_type":"text"},"cell_type":"markdown","source":["You must share only 1 folder with your tutor.\n","The folder should have your group's unikeys in its name ( example: Assignment1_abxy6273_edfr3373_yhfr4534 ).\n","\n","The folder should contain a Colab notebook and the output file.\n","\n","Suggested structure of the folder:\n","\n","* Data\n"," * training_data.csv\n"," * training_desc.csv\n"," * training_labels.csv\n"," * test_data.csv\n","* Assignment1.ipynb\n","* predicted_labels.csv\n","\n","Ask your tutor for his email address and share the properly named folder with him before 5:00pm on May 7th 2018. \n","Every late day will cost you 20 marks.\n","\n"]}]}